{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbcdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "from torch import optim\n",
    "from scipy.interpolate import CubicSpline\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f64438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "path_kp = \"data_files/kp_data.txt\"\n",
    "img_dir = \"data_files/solar_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d2f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kp data\n",
    "data_kp = pd.read_csv(path_kp)\n",
    "data_kp[\"datetime\"] = pd.to_datetime(data_kp[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ab5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames_unsorted = np.array(os.listdir(img_dir))\n",
    "image_names_split = (\n",
    "    pd.Series(image_filenames_unsorted).str.split(\".\").str[0].str.split(\"_\").str[:-2]\n",
    ")\n",
    "image_days = image_names_split.str[0]\n",
    "image_times = image_names_split.str[1]\n",
    "\n",
    "image_days_formatted = (\n",
    "    image_days.str[:4] + \"-\" + image_days.str[4:6] + \"-\" + image_days.str[6:]\n",
    ")\n",
    "image_times_formatted = (\n",
    "    image_times.str[:2] + \":\" + image_times.str[2:4] + \":\" + image_times.str[4:]\n",
    ")\n",
    "\n",
    "image_dates = pd.to_datetime(image_days_formatted + \" \" + image_times_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "076825f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series_sorted_image = image_dates.sort_values()\n",
    "image_dates_sort_idx = date_series_sorted_image.index.values\n",
    "\n",
    "image_dates_sorted = date_series_sorted_image.values\n",
    "image_filenames_sorted = image_filenames_unsorted[image_dates_sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ef3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_timestamps = image_dates_sorted.astype(\"int64\") // 10**9\n",
    "kp_timestamps = data_kp[\"datetime\"].values.astype(\"int64\") // 10**9\n",
    "kp_index_interpolated = CubicSpline(kp_timestamps, data_kp[\"Kp\"].values)(\n",
    "    image_timestamps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b81363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Image_filename</th>\n",
       "      <th>Kp</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>cycle_sin</th>\n",
       "      <th>cycle_cos</th>\n",
       "      <th>year_sin</th>\n",
       "      <th>year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1274400926</td>\n",
       "      <td>20100521_001526_1024_0171.jpg</td>\n",
       "      <td>4.344909</td>\n",
       "      <td>0.067290</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>-0.978951</td>\n",
       "      <td>0.204094</td>\n",
       "      <td>0.665272</td>\n",
       "      <td>-0.746601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1274411676</td>\n",
       "      <td>20100521_031436_1024_0171.jpg</td>\n",
       "      <td>1.121904</td>\n",
       "      <td>0.750688</td>\n",
       "      <td>0.660657</td>\n",
       "      <td>-0.972700</td>\n",
       "      <td>0.232065</td>\n",
       "      <td>0.663673</td>\n",
       "      <td>-0.748023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1274422452</td>\n",
       "      <td>20100521_061412_1024_0171.jpg</td>\n",
       "      <td>1.221935</td>\n",
       "      <td>0.998081</td>\n",
       "      <td>-0.061920</td>\n",
       "      <td>-0.965632</td>\n",
       "      <td>0.259913</td>\n",
       "      <td>0.662066</td>\n",
       "      <td>-0.749445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1274430876</td>\n",
       "      <td>20100521_083436_1024_0171.jpg</td>\n",
       "      <td>1.202672</td>\n",
       "      <td>0.780976</td>\n",
       "      <td>-0.624561</td>\n",
       "      <td>-0.959551</td>\n",
       "      <td>0.281534</td>\n",
       "      <td>0.660808</td>\n",
       "      <td>-0.750555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274439084</td>\n",
       "      <td>20100521_105124_1024_0171.jpg</td>\n",
       "      <td>0.665387</td>\n",
       "      <td>0.294874</td>\n",
       "      <td>-0.955536</td>\n",
       "      <td>-0.953161</td>\n",
       "      <td>0.302464</td>\n",
       "      <td>0.659581</td>\n",
       "      <td>-0.751633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp                 Image_filename        Kp   day_sin   day_cos  \\\n",
       "0  1274400926  20100521_001526_1024_0171.jpg  4.344909  0.067290  0.997733   \n",
       "1  1274411676  20100521_031436_1024_0171.jpg  1.121904  0.750688  0.660657   \n",
       "2  1274422452  20100521_061412_1024_0171.jpg  1.221935  0.998081 -0.061920   \n",
       "3  1274430876  20100521_083436_1024_0171.jpg  1.202672  0.780976 -0.624561   \n",
       "4  1274439084  20100521_105124_1024_0171.jpg  0.665387  0.294874 -0.955536   \n",
       "\n",
       "   cycle_sin  cycle_cos  year_sin  year_cos  \n",
       "0  -0.978951   0.204094  0.665272 -0.746601  \n",
       "1  -0.972700   0.232065  0.663673 -0.748023  \n",
       "2  -0.965632   0.259913  0.662066 -0.749445  \n",
       "3  -0.959551   0.281534  0.660808 -0.750555  \n",
       "4  -0.953161   0.302464  0.659581 -0.751633  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = pd.DataFrame(\n",
    "    {\n",
    "        \"Timestamp\": image_timestamps,\n",
    "        \"Image_filename\": image_filenames_sorted,\n",
    "        \"Kp\": kp_index_interpolated,\n",
    "    }\n",
    ")\n",
    "\n",
    "day = 24 * 60 * 60\n",
    "year = 365.2425 * day\n",
    "# Synodic carrington rotation of sun\n",
    "cycle = 27.2753 * day\n",
    "\n",
    "data_merged[\"day_sin\"] = np.sin(image_timestamps * (2 * np.pi / day))\n",
    "data_merged[\"day_cos\"] = np.cos(image_timestamps * (2 * np.pi / day))\n",
    "data_merged[\"cycle_sin\"] = np.sin(image_timestamps * (2 * np.pi / cycle))\n",
    "data_merged[\"cycle_cos\"] = np.cos(image_timestamps * (2 * np.pi / cycle))\n",
    "data_merged[\"year_sin\"] = np.sin(image_timestamps * (2 * np.pi / year))\n",
    "data_merged[\"year_cos\"] = np.cos(image_timestamps * (2 * np.pi / year))\n",
    "\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f125aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical data\n",
    "kp_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "timestamp_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "data_merged[\"Kp\"] = kp_scaler.fit_transform(data_merged[\"Kp\"].values.reshape(-1, 1))\n",
    "data_merged[\"Timestamp\"] = timestamp_scaler.fit_transform(data_merged[\"Timestamp\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3d92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        feature = data.iloc[i: i + seq_length]\n",
    "        target = data.iloc[i + seq_length][\"Kp\"]\n",
    "        sequences.append((feature, target))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f46f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAndKpDataset(Dataset):\n",
    "    def __init__(self, sequences, img_dir, img_transform=None):\n",
    "        self.sequences = sequences\n",
    "        self.img_transform = img_transform\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, target = self.sequences[idx]\n",
    "        images = []\n",
    "        numerical_features = []\n",
    "        for _, row in features.iterrows():\n",
    "            img_filename = row.pop(\"Image_filename\")\n",
    "            img_path = os.path.join(self.img_dir, img_filename)\n",
    "            image = read_image(img_path)\n",
    "            image = convert_image_dtype(image, torch.float32)\n",
    "            if self.img_transform:\n",
    "                image = self.img_transform(image)\n",
    "            images.append(image)\n",
    "            numerical_features.append(row.values)\n",
    "        images = torch.stack(images)\n",
    "        numerical_features = torch.tensor(\n",
    "            np.array(numerical_features).astype(float), dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        return images, numerical_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c234f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 7\n",
    "sequences = create_sequence(data_merged, seq_length)\n",
    "img_transform = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "dataset = ImageAndKpDataset(sequences, img_dir, img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78612cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation data:\n",
    "full_len = len(dataset)\n",
    "train_frac = 0.9\n",
    "train_size = int(full_len * train_frac)\n",
    "train_data = Subset(dataset, range(0, train_size))\n",
    "val_data = Subset(dataset, range(train_size, full_len))\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50782f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataloaders for data:\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2e1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarImageKpModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN for the solar images\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 150 * 150, 256)\n",
    "\n",
    "        # RNN for the image data\n",
    "        self.lstm_img = nn.LSTM(\n",
    "            input_size=256, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully-connected layers for the numerical data (8 input features)\n",
    "        self.fc_num1 = nn.Linear(8, 16)\n",
    "        self.fc_num2 = nn.Linear(16, 128)\n",
    "\n",
    "        # RNN for the numerical data\n",
    "        self.lstm_num = nn.LSTM(\n",
    "            input_size=128, hidden_size=128, num_layers=1, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully-connected layer that combines the image and numerical data to make the final prediction\n",
    "        self.fc_final = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x_img, x_num):\n",
    "        batch_size, seq_length, _, _, _ = x_img.size()\n",
    "\n",
    "        # Perform CNN\n",
    "        cnn_features = []\n",
    "        for i in range(seq_length):\n",
    "            img = x_img[:, i, :, :, :]\n",
    "            img = self.pool(F.relu(self.conv1(img)))\n",
    "            img = self.pool(F.relu(self.conv2(img)))\n",
    "            img = self.pool(F.relu(self.conv3(img)))\n",
    "            img = img.view(batch_size, -1)\n",
    "            img = F.relu(self.fc1(img))\n",
    "            cnn_features.append(img)\n",
    "\n",
    "        # Putting the CNN features together so we can pass them to the RNN\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "\n",
    "        rnn_out_img, _ = self.lstm_img(cnn_features)\n",
    "\n",
    "        # We take the last time step from the RNN\n",
    "        rnn_out_img = rnn_out_img[:, -1, :]\n",
    "\n",
    "        # The two fully-connected layers for the numerical features\n",
    "        x_num = F.relu(self.fc_num1(x_num))\n",
    "        x_num = F.relu(self.fc_num2(x_num))\n",
    "\n",
    "        # RNN for the numerical features\n",
    "        rnn_out_num, _ = self.lstm_num(x_num)\n",
    "        rnn_out_num = rnn_out_num[:, -1, :]\n",
    "\n",
    "        # Combining the CNN and numerical features RNN output\n",
    "        combined_rnn = torch.cat((rnn_out_img, rnn_out_num), dim=1)\n",
    "\n",
    "        # Final fully-connected layer\n",
    "        out = self.fc_final(combined_rnn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e01af990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SolarImageKpModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2880000, out_features=256, bias=True)\n",
       "  (lstm_img): LSTM(256, 128, batch_first=True)\n",
       "  (fc_num1): Linear(in_features=8, out_features=16, bias=True)\n",
       "  (fc_num2): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (lstm_num): LSTM(128, 128, batch_first=True)\n",
       "  (fc_final): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SolarImageKpModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca928515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(n_epochs, model, optimizer, loss_fn, train_loader, test_loader, device):\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch_img, batch_num, batch_target in train_loader:\n",
    "            batch_img, batch_num, batch_target = batch_img.to(device), batch_num.to(device), batch_target.to(device)\n",
    "            predictions = model(batch_img, batch_num)\n",
    "            loss = loss_fn(predictions, batch_target)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        # Calculate average training loss and accuracy\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        train_hist.append(average_loss)\n",
    "    \n",
    "        # Validation on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0.0\n",
    "    \n",
    "            for batch_img_test, batch_num_test, batch_target_test in test_loader:\n",
    "                batch_img_test, batch_num_test, batch_target_test = batch_img_test.to(device), batch_num_test.to(device), batch_target_test.to(device)\n",
    "                predictions_test = model(batch_img_test, batch_num_test)\n",
    "                test_loss = loss_fn(predictions_test, predictions_test)\n",
    "    \n",
    "                total_test_loss += test_loss.item()\n",
    "    \n",
    "            # Calculate average test loss and accuracy\n",
    "            average_test_loss = total_test_loss / len(test_loader)\n",
    "            test_hist.append(average_test_loss)\n",
    "        # if (epoch+1)%10==0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n",
    "    return train_hist, test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e71e556",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageAndKpDataset' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_hist, test_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtrain_valid\u001b[0;34m(n_epochs, model, optimizer, loss_fn, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mImageAndKpDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m convert_image_dtype(image, torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_transform:\n\u001b[0;32m---> 20\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m(image)\n\u001b[1;32m     21\u001b[0m images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m     22\u001b[0m numerical_features\u001b[38;5;241m.\u001b[39mappend(row\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageAndKpDataset' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "train_hist, test_hist = train_valid(n_epochs, model, optimizer, loss_fn, train_loader, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
