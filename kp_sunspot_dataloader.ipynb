{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./utilities/constants.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txt:\n",
    "    path_kp = os.path.join(DATA_DIR, \"kp_data.txt\")\n",
    "    path_sn = os.path.join(DATA_DIR, \"sn_data.txt\")\n",
    "else:\n",
    "    path_sn = os.path.join(DATA_DIR, \"SN_d_tot_V2.0.csv\") # data described: https://www.sidc.be/SILSO/newdataset\n",
    "    path_kp = os.path.join(DATA_DIR, \"kp_index.json\") # data described: https://kp.gfz-potsdam.de/en/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data under threshold: 4393, data over threshold: 1003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if txt:\n",
    "    data_kp = pd.read_csv(path_kp)\n",
    "    data_kp[\"datetime\"] = pd.to_datetime(data_kp[\"datetime\"])\n",
    "    data_kp[\"datetime\"] = data_kp[\"datetime\"].dt.floor(\"d\")\n",
    "    data_kp = data_kp.groupby(data_kp['datetime'].dt.date)['Kp'].max().reset_index()\n",
    "    data_kp[\"datetime\"] = pd.to_datetime(data_kp[\"datetime\"])\n",
    "else:\n",
    "    with open(path_kp, 'r') as f:\n",
    "        data_kp = json.load(f)\n",
    "    # convert to dataframe\n",
    "    data_kp = pd.DataFrame.from_dict(data_kp)\n",
    "    # convert datetime\n",
    "    data_kp[\"datetime\"] = pd.to_datetime(data_kp[\"datetime\"])\n",
    "    # normalize time resolution to one day inteval\n",
    "    data_kp = data_kp.groupby(data_kp['datetime'].dt.date)['Kp'].max().reset_index()\n",
    "    # convert Timestamp\n",
    "    data_kp[\"datetime\"] = pd.to_datetime(data_kp[\"datetime\"])\n",
    "\n",
    "# Transform: kp values to binary\n",
    "threshold = 5\n",
    "print(f\"data under threshold: {len(data_kp[data_kp[\"Kp\"] <= threshold])}, data over threshold: {len(data_kp[data_kp[\"Kp\"] >= threshold])}\")\n",
    "data_kp[\"Kp\"] = (data_kp[\"Kp\"] >= threshold).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txt:\n",
    "    data_sn = pd.read_csv(path_sn)\n",
    "    data_sn = data_sn.rename(columns={\"date\": \"datetime\", \"SN\":\"solar_spots_per_day\"})\n",
    "    data_sn[\"datetime\"] = pd.to_datetime(data_sn[\"datetime\"])\n",
    "else:\n",
    "    column_names = [\"year\", \"month\", \"day\", \"decimal_year\", \"SNvalue\" , \"SNerror\", \"Nb_observations\"]\n",
    "    data_sn = pd.read_csv(path_sn, sep=\";\", names=column_names, index_col=False)\n",
    "    data_sn = data_sn.rename(columns={\"Nb_observations\":\"solar_spots_per_day\"})\n",
    "    # extract and convert datetime\n",
    "    data_sn['datetime'] = pd.to_datetime(data_sn[['year', 'month', 'day']]) + pd.to_timedelta(data_sn['decimal_year'], unit='D')\n",
    "    data_sn = data_sn.drop(columns=[\"year\", \"month\", \"day\", \"decimal_year\"])\n",
    "    # filter data to max time resolution for start and end time\n",
    "    data_sn[\"datetime\"] = data_sn[\"datetime\"].dt.floor(\"d\")\n",
    "\n",
    "start_date = data_kp[\"datetime\"].iloc[0]\n",
    "end_date = data_kp[\"datetime\"].iloc[-1]\n",
    "data_sn = data_sn[(data_sn[\"datetime\"] >= start_date) & (data_sn[\"datetime\"] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2010-05-21\n",
       "1      2010-05-22\n",
       "2      2010-05-23\n",
       "3      2010-05-24\n",
       "4      2010-05-25\n",
       "          ...    \n",
       "5110   2024-05-17\n",
       "5111   2024-05-18\n",
       "5112   2024-05-19\n",
       "5113   2024-05-20\n",
       "5114   2024-05-21\n",
       "Name: datetime, Length: 5115, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sn[\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_mismatched_dates(df1, df2):\n",
    "    \"\"\"\n",
    "    Drop rows with mismatched dates between two dataframes.\n",
    "    \"\"\"\n",
    "    # Convert date columns to datetime if they're not already in datetime format\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df1[\"datetime\"]):\n",
    "        df1[\"datetime\"] = pd.to_datetime(df1[\"datetime\"])\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df2[\"datetime\"]):\n",
    "        df2[\"datetime\"] = pd.to_datetime(df2[\"datetime\"])\n",
    "\n",
    "    # Find dates that appear in one dataframe but not in the other\n",
    "    dates_in_df1_only = df1[~df1[\"datetime\"].isin(df2[\"datetime\"])]\n",
    "    dates_in_df2_only = df2[~df2[\"datetime\"].isin(df1[\"datetime\"])]\n",
    "\n",
    "    # Drop mismatched rows from both dataframes\n",
    "    df1_clean = df1.drop(dates_in_df1_only.index)\n",
    "    df2_clean = df2.drop(dates_in_df2_only.index)\n",
    "    \n",
    "    return df1_clean, df2_clean\n",
    "\n",
    "# remove mismatched data\n",
    "data_kp, data_sn = drop_mismatched_dates(data_kp, data_sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    False\n",
       "Kp          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure no NaN values\n",
    "data_kp.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime               False\n",
       "solar_spots_per_day    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sn.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on datetime\n",
    "df = pd.merge(data_kp, data_sn, on='datetime', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
